# GraphAI: Гибридный Слой Граф-Маскированного Внимания (DTG-MA)

**[English Version](README.md)** | **Russian Version**

> [!IMPORTANT]
> **Ключевая Идея**: Данный метод полностью решает проблему **Катастрофического Забывания (Catastrophic Forgetting)**. 
> Архитектура гарантирует, что обучение новым задачам **математически не может** повредить нейронные связи, отвечающие за старые знания. Это делает систему идеальной для *Непрерывного Обучения (Continual Learning)*.

## Обзор

**GraphAI** — это реализация архитектуры **Dynamic Topology Graph - Masked Attention (DTG-MA)** на языке Go. Проект демонстрирует подход к **Непрерывному Обучению (Continual Learning)**, интегрируя реальные LLM (через библиотеку `Cybertron`) с динамическим графовым адаптером.

Ключевые особенности:
*   **Динамическая Топология**: Структура графа расширяется по мере поступления новых задач/классов.
*   **Топологически-Осознанное Внимание**: Реализует формулу $Softmax(\frac{QK^T}{\sqrt{d}} + M_{task})V$, где $M_{task}$ использует "жесткую" маску ($-\infty$) для блокировки путей интерференции.
*   **Zero-Forgetting**: Параметры старых задач явно "замораживаются" (`Frozen` флаг в `DTGEdge`), что математически гарантирует отсутствие забывания.
*   **Реальные LLM**: Использует эмбеддинги от `BERT`, `MiniLM` и других моделей HuggingFace.

## Возможности

- **Полноценная Логика DTG-MA**:
  - **Метаданные Ребер**: Каждая матрица весов обернута в структуру `DTGEdge` (Weight, TaskID, Frozen).
  - **Строгое Маскирование**: Использование `-Inf` гарантирует, что внимание к запрещенным узлам равно строго 0.
  - **Управление Задачами**: Явное переключение контекста через `TaskID` в методе `Forward`.

- **Экосистема Pure Go**:
  - Построен на `Gorgonia` (граф вычислений).
  - Тензорные операции без Python-зависимостей.

## Сравнение с Аналогами (Why it's better)

В области Continual Learning существуют три классических подхода, каждый из которых имеет недостатки:

1.  **Elastic Weight Consolidation (EWC)**
    *   *Метод*: Использует матрицу Фишера для оценки важности весов и штрафует за изменение "важных" параметров.
    *   *Проблема*: Требует вычисления и хранения огромной матрицы Фишера, добавляя значительные вычислительные затраты. Не гарантирует 100% сохранение знаний (Soft Constraint).

2.  **Learning without Forgetting (LwF)**
    *   *Метод*: Применяет дистилляцию знаний, где старая модель служит "учителем" для новой.
    *   *Проблема*: Требует наличия доступа к старой модели и двойного прогона (учитель + ученик) при каждом обновлении, что замедляет обучение.

3.  **Parameter Isolation**
    *   *Метод*: Выделяет каждой задаче свои подсети или адаптеры.
    *   *Проблема*: Обычно масштабируется линейно и "глупо", просто увеличивая количество параметров без переиспользования знаний.

### Преимущество GraphAI (DTG-MA)
**GraphAI** решает эти проблемы, объединяя **Динамическую Топологию** и **Маскированное Внимание**:
*   **Эффективнее EWC**: Не требует вычисления матрицы Фишера. Защита знаний реализуется через *архитектурные* ограничения (`-Inf` Mask + Freezing), что вычислительно "бесплатно".
*   **Быстрее LwF**: Не требует прогона данных через старую модель (нет дистилляции).
*   **Умнее Isolation**: Вместо простого копирования сетей, использует граф, который может (в потенциале) переиспользовать узлы, а механизм `Masked Attention` позволяет динамически маршрутизировать сигналы, обеспечивая **Zero-Forgetting Guarantee** математически.

## Установка

### Требования
- Go 1.25+ (или настроенная переменная `ASSUME_NO_MOVING_GC_UNSAFE_RISK_IT_WITH=go1.25` для совместимости с Gorgonia/CUDA).

### Запуск
1.  Инициализация модуля:
    ```bash
    go mod init graphai
    go mod tidy
    ```
2.  Загрузка зависимостей:
    ```bash
    go get gorgonia.org/gorgonia
    go get github.com/nlpodyssey/cybertron
    ```

## Использование

### 1. Демо (MiniLM)
Быстрый запуск с легкой моделью `sentence-transformers/all-MiniLM-L6-v2`.
```bash
export ASSUME_NO_MOVING_GC_UNSAFE_RISK_IT_WITH=go1.25
go run main.go layer.go real_llm.go head.go
```

### 2. Масштабный Тест (BERT Base)
Запуск с моделью `bert-base-uncased` (768 измерений) и расширенным датасетом.
```bash
export ASSUME_NO_MOVING_GC_UNSAFE_RISK_IT_WITH=go1.25
go run run_large.go layer.go real_llm.go head.go
```
*Примечание: При первом запуске будет скачано около 440MB весов модели в папку `models/`.*

## Детали Архитектуры

### `layer.go`
Ядро реализации `HybridGraphLayer`.
- **`DTGEdge`**: Структура ребра `[Вес, ID Задачи, Заморожен]`.
- **`Forward(input, taskID)`**: Вычисляет маскированное внимание. Выбирает маску задачи и накладывает её аддитивно на Scores перед Softmax.
- **`FreezeOldTasks`**: Блокирует градиенты для задач $ID < Current$.

### `real_llm.go`
Обертка над `Cybertron`.
- **`NewRealLLM`**: Загружает модель HuggingFace.
- **`GetEmbeddings`**: Генерирует тензоры для графа.

## Конфигурация

Для настройки гиперпараметров (например, Learning Rate) измените создание солвера в `main.go`:
```go
solver := gorgonia.NewAdamSolver(gorgonia.WithLearnRate(0.001))
```

## Лицензия
MIT License.
